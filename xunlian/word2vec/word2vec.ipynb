{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1ce02ac5230>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed\n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 11:22:55,163 INFO: Fold lens [1091, 1091, 1091, 1091, 1091, 1091, 1091, 1091, 1091, 1091]\n"
     ]
    }
   ],
   "source": [
    "# split data to 10 fold\n",
    "fold_num = 10\n",
    "data_file = 'zhengti_fenci.csv'\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def all_data2fold(fold_num):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(data_file, encoding='UTF-8')\n",
    "    texts = f['text'].tolist()\n",
    "    labels = f['label'].tolist()\n",
    "\n",
    "    total = len(labels)\n",
    "\n",
    "    index = list(range(total))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        label = str(all_labels[i])\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        for i in range(fold_num):\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "\n",
    "    batch_size = int(total / fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "\n",
    "        if num > batch_size:\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size:\n",
    "            end = start + batch_size - num\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # shuffle\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "\n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "\n",
    "    return fold_data\n",
    "\n",
    "\n",
    "fold_data = all_data2fold(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-22-dde60de15aed>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[0mtrain_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;34m'label'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'text'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtrain_texts\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m \u001B[0mdev_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'./zhengti_test'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m \u001B[0mtrain_data\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'./zhengti_train'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'dict' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 11:22:55,192 INFO: Total 9819 docs.\n"
     ]
    }
   ],
   "source": [
    "# build train data for word2vec\n",
    "# 按照训练集来构建word2vec,所以是8份数据\n",
    "fold_id = 8\n",
    "\n",
    "train_texts = []\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "\n",
    "logging.info('Total %d docs.' % len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 11:22:55,246 INFO: Start training...\n",
      "2022-06-01 11:22:55,746 INFO: collecting all words and their counts\n",
      "2022-06-01 11:22:55,747 INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-06-01 11:22:56,497 INFO: collected 84615 word types from a corpus of 2302557 raw words and 9819 sentences\n",
      "2022-06-01 11:22:56,498 INFO: Creating a fresh vocabulary\n",
      "2022-06-01 11:22:56,715 INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 22549 unique words (26.65% of original 84615, drops 62066)', 'datetime': '2022-06-01T11:22:56.675171', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-06-01 11:22:56,716 INFO: Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 2202489 word corpus (95.65% of original 2302557, drops 100068)', 'datetime': '2022-06-01T11:22:56.716169', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-06-01 11:22:56,983 INFO: deleting the raw counts dictionary of 84615 items\n",
      "2022-06-01 11:22:56,988 INFO: sample=0.001 downsamples 47 most-common words\n",
      "2022-06-01 11:22:56,996 INFO: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1970055.7455593129 word corpus (89.4%% of prior 2202489)', 'datetime': '2022-06-01T11:22:56.996682', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'prepare_vocab'}\n",
      "2022-06-01 11:22:57,348 INFO: estimated required memory for 22549 words and 300 dimensions: 65392100 bytes\n",
      "2022-06-01 11:22:57,348 INFO: resetting layer weights\n",
      "2022-06-01 11:22:57,493 INFO: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-06-01T11:22:57.493081', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'build_vocab'}\n",
      "2022-06-01 11:22:57,494 INFO: Word2Vec lifecycle event {'msg': 'training model with 8 workers on 22549 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2022-06-01T11:22:57.494168', 'gensim': '4.2.0', 'python': '3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19044-SP0', 'event': 'train'}\n",
      "2022-06-01 11:22:58,932 INFO: EPOCH 0 - PROGRESS: at 7.02% examples, 103873 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:22:59,969 INFO: EPOCH 0 - PROGRESS: at 16.25% examples, 132400 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:01,383 INFO: EPOCH 0 - PROGRESS: at 24.01% examples, 124962 words/s, in_qsize 14, out_qsize 1\n",
      "2022-06-01 11:23:02,536 INFO: EPOCH 0 - PROGRESS: at 30.87% examples, 123064 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:03,539 INFO: EPOCH 0 - PROGRESS: at 38.29% examples, 126296 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:04,548 INFO: EPOCH 0 - PROGRESS: at 43.42% examples, 123471 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:05,694 INFO: EPOCH 0 - PROGRESS: at 50.96% examples, 123539 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:06,709 INFO: EPOCH 0 - PROGRESS: at 58.55% examples, 125520 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:07,774 INFO: EPOCH 0 - PROGRESS: at 66.74% examples, 128036 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:08,864 INFO: EPOCH 0 - PROGRESS: at 75.48% examples, 130617 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:09,943 INFO: EPOCH 0 - PROGRESS: at 83.21% examples, 132109 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:10,972 INFO: EPOCH 0 - PROGRESS: at 90.52% examples, 133300 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:11,980 INFO: EPOCH 0 - PROGRESS: at 99.46% examples, 136124 words/s, in_qsize 1, out_qsize 1\n",
      "2022-06-01 11:23:12,096 INFO: EPOCH 0: training on 2302557 raw words (1970611 effective words) took 14.5s, 135608 effective words/s\n",
      "2022-06-01 11:23:13,120 INFO: EPOCH 1 - PROGRESS: at 5.63% examples, 116285 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:14,498 INFO: EPOCH 1 - PROGRESS: at 13.88% examples, 115860 words/s, in_qsize 16, out_qsize 0\n",
      "2022-06-01 11:23:15,512 INFO: EPOCH 1 - PROGRESS: at 22.29% examples, 130627 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:16,522 INFO: EPOCH 1 - PROGRESS: at 29.66% examples, 133051 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:17,724 INFO: EPOCH 1 - PROGRESS: at 37.86% examples, 132965 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:18,731 INFO: EPOCH 1 - PROGRESS: at 45.67% examples, 136530 words/s, in_qsize 16, out_qsize 0\n",
      "2022-06-01 11:23:19,800 INFO: EPOCH 1 - PROGRESS: at 54.06% examples, 138236 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:20,873 INFO: EPOCH 1 - PROGRESS: at 62.16% examples, 138653 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:21,882 INFO: EPOCH 1 - PROGRESS: at 69.79% examples, 139779 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:22,973 INFO: EPOCH 1 - PROGRESS: at 76.31% examples, 137330 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:23,984 INFO: EPOCH 1 - PROGRESS: at 84.32% examples, 139850 words/s, in_qsize 15, out_qsize 0\n",
      "2022-06-01 11:23:25,060 INFO: EPOCH 1 - PROGRESS: at 91.38% examples, 139302 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-21-c9b9149c9616>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;31m# 采用skip-gram，负采样，训练10轮，12核CPU大约需要4个小时\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m model = Word2Vec(train_texts, sg=1, workers=num_workers, vector_size=num_features, compute_loss=True,\n\u001B[0m\u001B[0;32m     11\u001B[0m                  epochs=10, hs=0, window=10)\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[0;32m    425\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_corpus_sanity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus_file\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpasses\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    426\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuild_vocab\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus_file\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrim_rule\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrim_rule\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 427\u001B[1;33m             self.train(\n\u001B[0m\u001B[0;32m    428\u001B[0m                 \u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorpus_file\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcorpus_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_examples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus_count\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    429\u001B[0m                 \u001B[0mtotal_words\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcorpus_total_words\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstart_alpha\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0malpha\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m   1068\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1069\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mcorpus_iterable\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1070\u001B[1;33m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001B[0m\u001B[0;32m   1071\u001B[0m                     \u001B[0mcorpus_iterable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcur_epoch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcur_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_examples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtotal_examples\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1072\u001B[0m                     \u001B[0mtotal_words\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtotal_words\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mqueue_factor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mqueue_factor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreport_delay\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreport_delay\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36m_train_epoch\u001B[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001B[0m\n\u001B[0;32m   1429\u001B[0m             \u001B[0mthread\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1430\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1431\u001B[1;33m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001B[0m\u001B[0;32m   1432\u001B[0m             \u001B[0mprogress_queue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mjob_queue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcur_epoch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcur_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_examples\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtotal_examples\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1433\u001B[0m             \u001B[0mtotal_words\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtotal_words\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreport_delay\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreport_delay\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mis_corpus_file_mode\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py\u001B[0m in \u001B[0;36m_log_epoch_progress\u001B[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001B[0m\n\u001B[0;32m   1284\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1285\u001B[0m         \u001B[1;32mwhile\u001B[0m \u001B[0munfinished_worker_count\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1286\u001B[1;33m             \u001B[0mreport\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprogress_queue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# blocks if workers too slow\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1287\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mreport\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# a thread reporting that it finished\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1288\u001B[0m                 \u001B[0munfinished_worker_count\u001B[0m \u001B[1;33m-=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\nlp\\lib\\queue.py\u001B[0m in \u001B[0;36mget\u001B[1;34m(self, block, timeout)\u001B[0m\n\u001B[0;32m    168\u001B[0m             \u001B[1;32melif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    169\u001B[0m                 \u001B[1;32mwhile\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_qsize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 170\u001B[1;33m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnot_empty\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    171\u001B[0m             \u001B[1;32melif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"'timeout' must be a non-negative number\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\anaconda3\\envs\\nlp\\lib\\threading.py\u001B[0m in \u001B[0;36mwait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    300\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m    \u001B[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    301\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 302\u001B[1;33m                 \u001B[0mwaiter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    303\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    304\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "logging.info('Start training...')\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "num_features = 300     # Word vector dimensionality\n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "\n",
    "train_texts = list(map(lambda x: list(x.split()), train_texts))\n",
    "\n",
    "# 采用skip-gram，负采样，训练10轮，12核CPU大约需要4个小时\n",
    "model = Word2Vec(train_texts, sg=1, workers=num_workers, vector_size=num_features, compute_loss=True,\n",
    "                 epochs=10, hs=0, window=10)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save model\n",
    "model.save(\"./word2vec.bin\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load model\n",
    "model = Word2Vec.load(\"./word2vec.bin\")\n",
    "\n",
    "# convert format\n",
    "model.wv.save_word2vec_format('./word2vec.txt', binary=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (shihua)",
   "language": "python",
   "name": "pycharm-f70e0637"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}